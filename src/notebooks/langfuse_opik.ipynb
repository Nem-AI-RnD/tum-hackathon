{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6db07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pprint import PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc3ae75",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m llm = ChatVertexAI(model_name=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.0-flash-001\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0.2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTell me a joke\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:307\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m     **kwargs: Any,\n\u001b[32m    303\u001b[39m ) -> BaseMessage:\n\u001b[32m    304\u001b[39m     config = ensure_config(config)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    306\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    317\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:843\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    836\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    837\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     **kwargs: Any,\n\u001b[32m    841\u001b[39m ) -> LLMResult:\n\u001b[32m    842\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:1286\u001b[39m, in \u001b[36mChatVertexAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_gemini_model:\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_non_gemini(messages, stop=stop, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_gemini\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:1522\u001b[39m, in \u001b[36mChatVertexAI._generate_gemini\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_gemini\u001b[39m(\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1516\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1519\u001b[39m     **kwargs: Any,\n\u001b[32m   1520\u001b[39m ) -> ChatResult:\n\u001b[32m   1521\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request_gemini(messages=messages, stop=stop, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1522\u001b[39m     response = \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gemini_response_to_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:629\u001b[39m, in \u001b[36m_completion_with_retry\u001b[39m\u001b[34m(generation_method, max_retries, run_manager, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    624\u001b[39m params = (\n\u001b[32m    625\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mis_gemini\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    628\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:622\u001b[39m, in \u001b[36m_completion_with_retry.<locals>._completion_with_retry_inner\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_completion_with_retry_inner\u001b[39m(generation_method: Callable, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py:2396\u001b[39m, in \u001b[36mPredictionServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m   2393\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m   2395\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2396\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/transports/grpc.py:84\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     69\u001b[39m     grpc_request = {\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     73\u001b[39m     }\n\u001b[32m     74\u001b[39m     _LOGGER.debug(\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m         },\n\u001b[32m     82\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     86\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/grpc/_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m   1192\u001b[39m     (\n\u001b[32m   1193\u001b[39m         state,\n\u001b[32m   1194\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/grpc/_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2)\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74a7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    " \n",
    "# Initialize Langfuse client (prompt management)\n",
    "langfuse = Langfuse()\n",
    " \n",
    "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "langfuse_callback_handler = CallbackHandler()\n",
    " \n",
    "# Optional, verify that Langfuse is configured correctly\n",
    "assert langfuse.auth_check()\n",
    "assert langfuse_callback_handler.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c302e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 validation errors for UpdateGenerationBody\n",
      "usageDetails -> prompt_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> candidates_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> cache_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> prompt_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> completion_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> prompt_tokens_details -> modality\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> input_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> output_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/avosseler/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langfuse/client.py\", line 2733, in update\n",
      "    request = UpdateGenerationBody(**generation_body)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/avosseler/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/pydantic/v1/main.py\", line 341, in __init__\n",
      "    raise validation_error\n",
      "pydantic.v1.error_wrappers.ValidationError: 10 validation errors for UpdateGenerationBody\n",
      "usageDetails -> prompt_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> candidates_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> cache_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> prompt_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> completion_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> prompt_tokens_details -> modality\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> input_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> output_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Golden Doodles is a city in the United States. It is located in the state of Florida.\\n' additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 10, 'candidates_token_count': 21, 'total_token_count': 31, 'prompt_tokens_details': [{'modality': 1, 'token_count': 10}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 21}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.5778875350952148, 'model_name': 'gemini-2.0-flash-001'} id='run-7402ea44-d2b2-486d-a044-57f36e7bb6e8-0' usage_metadata={'input_tokens': 10, 'output_tokens': 21, 'total_tokens': 31}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "model = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"person\": \"Grace Hopper\"}, config={\n",
    "    \"callbacks\":[langfuse_callback_handler]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4141094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 validation errors for UpdateGenerationBody\n",
      "usageDetails -> prompt_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> candidates_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> cache_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> prompt_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> completion_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> prompt_tokens_details -> modality\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> input_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> output_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/avosseler/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langfuse/client.py\", line 2733, in update\n",
      "    request = UpdateGenerationBody(**generation_body)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/avosseler/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/pydantic/v1/main.py\", line 341, in __init__\n",
      "    raise validation_error\n",
      "pydantic.v1.error_wrappers.ValidationError: 10 validation errors for UpdateGenerationBody\n",
      "usageDetails -> prompt_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> candidates_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> cache_tokens_details\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> prompt_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> completion_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> prompt_tokens_details -> modality\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "usageDetails -> input_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> output_tokens\n",
      "  field required (type=value_error.missing)\n",
      "usageDetails -> total_tokens\n",
      "  field required (type=value_error.missing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Grace Hopper was born in **New York City**.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "model = VertexAI(model_name=\"gemini-2.0-flash-001\")\n",
    "\n",
    "# Initialize callback handler\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# pass langfuse_handler as callback to `invoke`\n",
    "chain.invoke({\"person\": \"Grace Hopper\"}, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111e26d",
   "metadata": {},
   "source": [
    "# Opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985a51c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"A.I. Assistant Evaluation ðŸ¤–\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019663a8-c119-7971-ad09-0dc9bbeb8a03&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Why did the zebra cross the road?\\n\\nTo prove he wasn't chicken! He just wanted to show he had the stripes to do it!\\n\" additional_kwargs={} response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 6, 'candidates_token_count': 31, 'total_token_count': 37, 'prompt_tokens_details': [{'modality': 1, 'token_count': 6}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 31}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.16969050130536478, 'model_name': 'gemini-2.0-flash-001'} id='run-b49846b2-2293-42a1-9637-b1fda131be0e-0' usage_metadata={'input_tokens': 6, 'output_tokens': 31, 'total_tokens': 37}\n",
      "Tell a joke about {input}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import opik\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "from ai_eval.config import global_config as glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "opik_tracer = OpikTracer(project_name=glob.OPIK_PROJECT_NAME)\n",
    "\n",
    "llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2)\n",
    "\n",
    "# llm = ChatOllama(model=\"llama3.2\", temperature=0.2)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"], template=\"Tell a joke about {input}\"\n",
    ")\n",
    "\n",
    "llm_chain = (prompt_template | llm).with_config({\"callbacks\": [opik_tracer]})\n",
    "\n",
    "input_text = \"Zebras\"\n",
    "\n",
    "print(llm_chain.invoke({\"input\": input_text})) \n",
    "\n",
    "#opik_tracer.flush()\n",
    "\n",
    "## Write your Prompt to the Prompt Library:\n",
    "## ----------------------------------------\n",
    "#my_prompt = opik.Prompt(name=\"tell me\", prompt=prompt_template.format(input=input_text))\n",
    "my_prompt = opik.Prompt(name=\"tell me\", prompt=prompt_template.template, metadata={\"input\": input_text})\n",
    "\n",
    "# Use the prompt in your application (e.g., for evaluation tasks)\n",
    "print(my_prompt.prompt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdae421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Tell a joke about {input}')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd383476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from ai_eval.utils.utils import extract_prompt\n",
    "\n",
    "#@extract_prompt()\n",
    "@extract_prompt(prompt_libname=\"Prompt Test\")\n",
    "def chat_llm(prompt: PromptTemplate, llm: LLM, **inputs: str) -> str:\n",
    "    \"\"\"\n",
    "    Call a language model with a given prompt template and inputs.\n",
    "\n",
    "    This function processes a prompt through a language model and returns the response as a string.\n",
    "\n",
    "    Args:\n",
    "        prompt (PromptTemplate): The template to format with input variables\n",
    "        llm (LLM): The language model to process the prompt\n",
    "        **inputs: Variable keyword arguments to populate the prompt template\n",
    "\n",
    "    Returns:\n",
    "        str: The processed response from the language model\n",
    "\n",
    "    Example:\n",
    "        >>> prompt = PromptTemplate(\"Answer this question: {question}\")\n",
    "        >>> llm = ChatOpenAI()\n",
    "        >>> response = chat_llm(prompt, llm, question=\"What is 2+2?\")\n",
    "    \"\"\"\n",
    "\n",
    "    question_router = prompt | llm | StrOutputParser()\n",
    "    return question_router.invoke(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3dbad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Opik Prompt: <opik.api_objects.prompt.prompt.Prompt object at 0x13ebdf2c0>\n",
      "Why did Alex bring a ladder to the party?\n",
      "\n",
      "Because he heard the drinks were on the house! (get it?)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"], \n",
    "    template=\"Tell a joke about {input}\"\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.2)\n",
    "\n",
    "# Call the decorated function\n",
    "response = chat_llm(prompt=prompt_template, llm=llm, input=\"Alex\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b21ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation import evaluate_prompt\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from ai_eval.config import global_config as glob\n",
    "\n",
    "# Initialize Opik Tracer for logging\n",
    "opik_tracer = OpikTracer(project_name=glob.OPIK_PROJECT_NAME)\n",
    "\n",
    "# Initialize the ChatVertexAI model\n",
    "llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2).with_config(\n",
    "    {\"callbacks\": [opik_tracer]}\n",
    ")\n",
    "\n",
    "# Define the evaluation prompt\n",
    "prompt = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Translate the following text to French: {{input}}\",\n",
    "}\n",
    "\n",
    "# Define the input for the prompt\n",
    "input_data = {\"input\": \"Hello, how are you?\"}\n",
    "\n",
    "# Run the evaluation prompt with Opik tracing\n",
    "response = evaluate_prompt(\n",
    "    project_name=glob.OPIK_PROJECT_NAME,\n",
    "    model=llm,\n",
    "    prompt=prompt,\n",
    "    input_data=input_data,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a80a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Created a \"My dataset\" dataset at https://www.comet.com/opik/api/v1/session/redirect/datasets/?dataset_id=01962a19-3d54-75b3-aae8-cf1a7a4d82b6&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    }
   ],
   "source": [
    "from opik import Opik\n",
    "\n",
    "# Create a dataset\n",
    "client = Opik(project_name=os.getenv(\"OPIK_PROJECT_NAME\"))\n",
    "\n",
    "dataset = client.get_or_create_dataset(name=\"My dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023c7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset items to it\n",
    "dataset.insert([\n",
    "    {\"user_question\": \"Hello, world!\", \"expected_output\": {\"assistant_answer\": \"Hello, world!\"}},\n",
    "    {\"user_question\": \"What is the capital of France?\", \"expected_output\": {\"assistant_answer\": \"Paris\"}},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting items from a JSONL file\n",
    "dataset.read_jsonl_from_file(\"path/to/file.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb4e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting items from a Pandas DataFrame\n",
    "dataset.insert_from_pandas(dataframe=df)\n",
    "\n",
    "# dataset.insert_from_pandas(dataframe=df, keys_mapping={\"Expected output\": \"expected_output\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc453b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import Opik\n",
    "\n",
    "# Get or create a dataset\n",
    "dataset = client.get_dataset(name=\"My dataset\")\n",
    "\n",
    "# delete items from the dataset\n",
    "# delete by item id\n",
    "dataset.delete(items_ids=[\"123\", \"456\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all items from a dataset\n",
    "dataset = client.get_dataset(name=\"My dataset\")\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "dataset.to_pandas()\n",
    "\n",
    "# Convert to a JSON array\n",
    "dataset.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98850e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_eval.services.opik import OpikService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cbab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_service = OpikService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae2cca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n  {\\n    \"expected_output\": {\\n      \"assistant_answer\": \"Paris\"\\n    },\\n    \"user_question\": \"What is the capital of France?\",\\n    \"id\": \"01962a1a-7457-706b-9000-c3c53284c9ea\"\\n  },\\n  {\\n    \"expected_output\": {\\n      \"assistant_answer\": \"Hello, world!\"\\n    },\\n    \"user_question\": \"Hello, world!\",\\n    \"id\": \"01962a1a-7456-71b3-9f83-8bff63d5fb20\"\\n  }\\n]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opik_service.read(name=\"My dataset\", output_format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce35836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:54:23,914 - ai_eval.services.opik - INFO - Creating Opik dataset 'test'\n",
      "OPIK: Created a \"test\" dataset at https://www.comet.com/opik/api/v1/session/redirect/datasets/?dataset_id=01962a47-b3fa-7f35-96f7-9e883dfe71b4&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<opik.api_objects.dataset.dataset.Dataset at 0x120232b70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opik_service.create(name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c5b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\\nPlay Synopsis:\\ncontent=\\'## The Yellow Ball\\\\n\\\\n**Synopsis:**\\\\n\\\\n\"The Yellow Ball\" is a heart-wrenching drama set against the backdrop of a breathtaking, yet ultimately cruel, sunset on a seemingly idyllic beach. The play opens with five-year-old Sarah happily playing with her father, David, a kind and loving man. Their joy is shattered when Sarah\\\\\\'s yellow ball is carried out to sea. David, without hesitation, wades into the water to retrieve it.\\\\n\\\\nAs the sun dips below the horizon, painting the sky in fiery hues, a sense of unease descends. David doesn\\\\\\'t return. Panic erupts as lifeguards and volunteers launch a frantic search. Sarah, wrapped in a blanket and comforted by a stranger, repeats, \"Daddy went to get my ball.\"\\\\n\\\\nThe play follows the desperate search, highlighting the growing fear and the community\\\\\\'s collective grief. The yellow ball, bobbing innocently on the waves, becomes a symbol of lost innocence and shattered hope.\\\\n\\\\nUltimately, David\\\\\\'s lifeless body is recovered from the sea. The play explores the devastating impact of his death on Sarah, her family, and the community. It delves into themes of loss, grief, the fragility of life, and the deceptive beauty of nature.\\\\n\\\\n\"The Yellow Ball\" is a poignant exploration of a family\\\\\\'s tragedy, a stark reminder of the sea\\\\\\'s power, and the enduring impact of a single, fateful moment. The play leaves the audience contemplating the unpredictable nature of life and the enduring power of love in the face of unimaginable loss.\\\\n\\' additional_kwargs={} response_metadata={\\'is_blocked\\': False, \\'safety_ratings\\': [], \\'usage_metadata\\': {\\'prompt_token_count\\': 838, \\'candidates_token_count\\': 317, \\'total_token_count\\': 1155, \\'prompt_tokens_details\\': [{\\'modality\\': 1, \\'token_count\\': 838}], \\'candidates_tokens_details\\': [{\\'modality\\': 1, \\'token_count\\': 317}], \\'cached_content_token_count\\': 0, \\'cache_tokens_details\\': []}, \\'finish_reason\\': \\'STOP\\', \\'avg_logprobs\\': -0.19472006668427766, \\'model_name\\': \\'gemini-2.0-flash-001\\'} id=\\'run-f85fa19c-a330-47a6-aa59-2a15387da4ee-0\\' usage_metadata={\\'input_tokens\\': 838, \\'output_tokens\\': 317, \\'total_tokens\\': 1155}\\nReview from a New York Times play critic of the above play:'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2)\n",
    "\n",
    "# Initialize Opik tracer\n",
    "opik_tracer = OpikTracer(tags=[\"LangChain Example\"])\n",
    "\n",
    "# Synopsis chain\n",
    "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "            Title: {title}\n",
    "            Playwright: This is a synopsis for the above play:\"\"\"\n",
    "            \n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "\n",
    "synopsis_chain = llm | prompt_template\n",
    "\n",
    "# Review chain\n",
    "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "#review_chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "\n",
    "review_chain = llm | prompt_template\n",
    "\n",
    "# Overall chain\n",
    "overall_chain = (synopsis_chain | review_chain).with_config({\"callbacks\": [opik_tracer]})\n",
    "\n",
    "# Run the chain with Opik tracing\n",
    "review = overall_chain.invoke(\"Tragedy at sunset on the beach\")\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6a60ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T11:51:29.395275Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2202414833, 'load_duration': 1735034125, 'prompt_eval_count': 29, 'prompt_eval_duration': 265239584, 'eval_count': 18, 'eval_duration': 201202916, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-f478a7c4-2080-4e7e-b3bd-2b34f3bde04e-0', usage_metadata={'input_tokens': 29, 'output_tokens': 18, 'total_tokens': 47})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22072bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je t\\'aime, Opik.\\n\\n(Note: \"Opik\" is likely a nickname or a term of endearment, and the translation is an expression of affection.)', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T12:10:42.843348Z', 'done': True, 'done_reason': 'stop', 'total_duration': 555649416, 'load_duration': 29182875, 'prompt_eval_count': 47, 'prompt_eval_duration': 95598000, 'eval_count': 36, 'eval_duration': 429741875, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1284d21d-10f1-4d76-864f-c336aba0cd9e-0', usage_metadata={'input_tokens': 47, 'output_tokens': 36, 'total_tokens': 83})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "from opik import Opik, track\n",
    "from ai_eval.config import global_config as glob\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the Opik tracer\n",
    "opik_tracer = OpikTracer(project_name=glob.OPIK_PROJECT_NAME, tags=[\"langchain\", \"ollama\"])\n",
    "\n",
    "# Create the Ollama model and configure it to use the Opik tracer\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ").with_config({\"callbacks\": [opik_tracer]})\n",
    "\n",
    "\n",
    "# llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2).with_config({\"callbacks\": [opik_tracer]})\n",
    "\n",
    "# Call the Ollama model\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"I love you Opik.\",\n",
    "    ),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3cfab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpikTracer' object has no attribute 'create_nested_span'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m nested_span_config = {\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproject_name\u001b[39m\u001b[33m\"\u001b[39m: glob.OPIK_PROJECT_NAME\n\u001b[32m     15\u001b[39m }\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Create the nested span with the specified project name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mopik_tracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_nested_span\u001b[49m(name=\u001b[33m\"\u001b[39m\u001b[33mNested Span\u001b[39m\u001b[33m\"\u001b[39m, config=nested_span_config)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create the Ollama model and configure it to use the Opik tracer\u001b[39;00m\n\u001b[32m     21\u001b[39m llm = ChatOllama(\n\u001b[32m     22\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mllama3.2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     temperature=\u001b[32m0\u001b[39m,\n\u001b[32m     24\u001b[39m ).with_config({\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: [opik_tracer]})\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpikTracer' object has no attribute 'create_nested_span'"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "from opik import Opik, track\n",
    "from ai_eval.config import global_config as glob\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the Opik tracer\n",
    "opik_tracer = OpikTracer(project_name=glob.OPIK_PROJECT_NAME, tags=[\"langchain\", \"ollama\"])\n",
    "\n",
    "# Create the Ollama model and configure it to use the Opik tracer\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ").with_config({\"callbacks\": [opik_tracer]})\n",
    "\n",
    "\n",
    "# Define your LLM application function\n",
    "@track\n",
    "def your_llm_application(input_text: str) -> str:\n",
    "    # Call the Ollama model\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            input_text,\n",
    "        ),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage of the LLM application\n",
    "input_text = \"How can we improve language model evaluations?\"\n",
    "output_text = your_llm_application(input_text)\n",
    "\n",
    "#print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15868fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatVertexAI' from 'langchain_openai' (/Users/avosseler/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_openai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopik\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Opik, track\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ChatVertexAI' from 'langchain_openai' (/Users/avosseler/Github/Team/ai-assistant-eval/.venv/lib/python3.12/site-packages/langchain_openai/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatVertexAI\n",
    "from opik import Opik, track\n",
    "from langchain.prompts import PromptTemplate\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "# Initialize Opik Tracer\n",
    "opik_tracer = OpikTracer(tags=[\"ChatVertexAI Experiment\"], project_name=glob.OPIK_PROJECT_NAME)\n",
    "\n",
    "# Initialize ChatVertexAI model\n",
    "llm = ChatVertexAI(model_name=\"gemini-2.0-flash-001\", temperature=0.2).with_config({\"callbacks\": [opik_tracer]})\n",
    "\n",
    "# Define your LLM application function\n",
    "@track\n",
    "def your_llm_application(input_text: str) -> str:\n",
    "    response = llm.generate_response(input_text)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage of the LLM application\n",
    "input_text = \"How can we improve language model evaluations?\"\n",
    "output_text = your_llm_application(input_text)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a38075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the project name in the nested span configuration\n",
    "nested_span_config = {\n",
    "    \"project_name\": \"My First Project\"\n",
    "}\n",
    "\n",
    "# Create the nested span with the specified project name\n",
    "opik_tracer.create_nested_span(name=\"Nested Span\", config=nested_span_config)\n",
    "\n",
    "# Log data within the nested span\n",
    "opik_tracer.log_data(\"Data to be logged in the nested span\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-assistant-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
