{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¨ Dual Evaluator RAG Pipeline Evaluation Notebook\n",
        "\n",
        "## Purpose\n",
        "This notebook allows **two independent human evaluators** (A and B) to assess a Retrieval-Augmented Generation (RAG) pipeline in parallel.\n",
        "\n",
        "## Goals\n",
        "- **Independent, unbiased human evaluation** from two perspectives\n",
        "- **Structured scoring and notes** for each query using a standardized rubric\n",
        "- **Later comparison** between evaluators and optional automated metrics\n",
        "\n",
        "## Workflow\n",
        "1. ‚úÖ **Run Setup** (pipeline + environment)\n",
        "2. üë§ **Evaluator A** completes their section independently\n",
        "3. üë§ **Evaluator B** completes their section independently\n",
        "4. ü§ñ **Run automated evaluation** (e.g., DeepEval) if desired\n",
        "5. üîç **Compare results** and discuss insights\n",
        "\n",
        "## ‚ö†Ô∏è Independence & Blindness Notice\n",
        "- **Evaluator A and B must NOT see each other's answers or scores before finishing**\n",
        "- **Automated evaluation should NOT be shown to either evaluator ahead of time**\n",
        "- Each evaluator works in their own dedicated section\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1Ô∏è‚É£ Setup: Environment & API Keys\n",
        "\n",
        "## Purpose\n",
        "This section is for the **organizer/technical person**, not the evaluators.\n",
        "\n",
        "## What This Section Does\n",
        "- ‚úÖ Verifies required API keys (language model, vector DB, RAG backend)\n",
        "- ‚úÖ Initializes the RAG pipeline (similar to main.py)\n",
        "- ‚úÖ Runs a simple test query to confirm the pipeline works\n",
        "\n",
        "## Requirements\n",
        "You must have the following environment variables set:\n",
        "- `ANTHROPIC_API_KEY` - For Claude LLM\n",
        "- `JINA_API_KEY` - For embeddings\n",
        "- `QDRANT_API_KEY` - For vector database\n",
        "- `QDRANT_URL` - Qdrant instance URL\n",
        "\n",
        "## Success Criteria\n",
        "After running setup cells, you should see:\n",
        "- ‚úÖ \"All required API keys loaded successfully!\"\n",
        "- ‚úÖ \"RAG pipeline fully configured and ready!\"\n",
        "- ‚úÖ Test query returns an answer with context\n",
        "\n",
        "## ‚ö†Ô∏è Important\n",
        "**Do not modify setup cells after evaluators have started their work!**\n",
        "\n",
        "---\n",
        "\n",
        "## Import Dependencies and Load API Keys\n",
        "\n",
        "**Instructions**: Run this cell to import all required libraries and validate API keys.\n",
        "\n",
        "# Configure Anthropic multimodal LLM (Claude 3) for image understanding\n",
        "# Requires: pip install llama-index-multi-modal-llms-anthropic\n",
        "anthropic_mm_llm = AnthropicMultiModal(\n",
        "    model=\"claude-3-sonnet-20240229\",  # or \"claude-3-opus-20240229\"\n",
        "    max_tokens=300,\n",
        ")\n",
        "\n",
        "print(\"\\u2705 Anthropic multimodal LLM configured (Claude 3 Sonnet)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/karl/Documents/tum-hackathon/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/karl/Documents/tum-hackathon/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All required API keys loaded successfully!\n",
            "üìÖ Session started at: 2025-11-22 15:16:01\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "from typing import List, Any, Dict, Tuple\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import qdrant_client\n",
        "from dotenv import load_dotenv\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from ai_eval.resources import deepeval_scorer as deep\n",
        "from ai_eval.resources.rag_template import RAG\n",
        "from ai_eval.resources import eval_dataset_builder as eval_builder\n",
        "from ai_eval.services.file import JSONService\n",
        "from ai_eval.config import global_config as glob\n",
        "\n",
        "from llama_index.core import (\n",
        "    Document as LlamaIndexDocument,\n",
        "    Settings,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.jinaai import JinaEmbedding\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.response_synthesizers import ResponseMode\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Validate API keys\n",
        "required_keys = {\n",
        "    \"ANTHROPIC_API_KEY\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "    \"JINA_API_KEY\": os.getenv(\"JINA_API_KEY\"),\n",
        "    \"QDRANT_API_KEY\": os.getenv(\"QDRANT_API_KEY\"),\n",
        "    \"QDRANT_URL\": os.getenv(\"QDRANT_URL\"),\n",
        "    \"LLAMAPARSE_API_KEY\": os.getenv(\"LLAMAPARSE_API_KEY\"),\n",
        "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "}\n",
        "\n",
        "missing_keys = [key for key, value in required_keys.items() if not value]\n",
        "if missing_keys:\n",
        "    raise ValueError(\n",
        "        f\"‚ùå Missing required API keys: {', '.join(missing_keys)}. Please set them in your .env file.\")\n",
        "\n",
        "# Optional: LlamaCloud API key\n",
        "llamacloud_api_key = os.getenv(\"LLAMACLOUD_API_KEY\")\n",
        "if llamacloud_api_key:\n",
        "    os.environ[\"LLAMA_CLOUD_API_KEY\"] = llamacloud_api_key\n",
        "\n",
        "print(\"‚úÖ All required API keys loaded successfully!\")\n",
        "print(f\"üìÖ Session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Configure Embedding Model\n",
        "\n",
        "**Instructions**: Configure Jina AI embeddings for document and query encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Jina AI embeddings configured (model: jina-embeddings-v4)\n",
            "üìè Embedding dimension (vector size): 2048\n",
            "üìä Model: multimodal & multilingual, retrieval-optimized\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Explicitly configure the embedding vector dimension\n",
        "\n",
        "VECTOR_DIM = 2048\n",
        "\n",
        "\n",
        "\n",
        "# Configure Jina AI embeddings (Jina v4, retrieval-optimized, multimodal-capable)\n",
        "\n",
        "# Get your Jina AI API key for free: https://jina.ai/?sui=apikey\n",
        "\n",
        "embed_model = JinaEmbedding(\n",
        "\n",
        "    api_key=required_keys[\"JINA_API_KEY\"],\n",
        "\n",
        "    model=\"jina-embeddings-v4\",   # ensure this model is 2048-dim or adjust VECTOR_DIM accordingly\n",
        "\n",
        "    task=\"retrieval.passage\",\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Register embedding model with LlamaIndex\n",
        "\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "\n",
        "\n",
        "# Validate that the model dimension matches VECTOR_DIM (fail fast if not)\n",
        "\n",
        "actual_dim = getattr(embed_model, \"dimension\", None)\n",
        "\n",
        "if actual_dim is None:\n",
        "\n",
        "    actual_dim = getattr(embed_model, \"embed_dim\", None)\n",
        "\n",
        "\n",
        "\n",
        "if actual_dim is not None and actual_dim != VECTOR_DIM:\n",
        "\n",
        "    raise ValueError(\n",
        "\n",
        "        f\"Embedding model dimension ({actual_dim}) does not match configured VECTOR_DIM ({VECTOR_DIM}).\\n\"\n",
        "\n",
        "        f\"Please switch to a 2048-dimensional Jina embedding model or update VECTOR_DIM to {actual_dim}.\"\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "embedding_dim = VECTOR_DIM\n",
        "\n",
        "\n",
        "\n",
        "print(\"‚úÖ Jina AI embeddings configured (model: jina-embeddings-v4)\")\n",
        "\n",
        "print(f\"üìè Embedding dimension (vector size): {embedding_dim}\")\n",
        "\n",
        "print(\"üìä Model: multimodal & multilingual, retrieval-optimized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Load and Process Documents\n",
        "\n",
        "**Instructions**: Load the Allplan PDF manual and convert it to LlamaIndex format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started parsing the file under job_id 907080ca-00c6-4815-bea4-51724742b6cc\n",
            "..üìÑ Loaded 307 pages from PDF (multimodal LlamaParse)\n",
            "‚úÖ Converted 307 pages to LlamaIndex format\n"
          ]
        }
      ],
      "source": [
        "# Load PDF document using LlamaParse with Claude 3.5 Sonnet (multimodal parsing)\n",
        "filename = \"../../data/Allplan_2020_Manual.pdf\"\n",
        "loader = LlamaParse(\n",
        "\n",
        "    parse_mode=\"parse_page_with_lvm\",          # page-level multimodal parsing\n",
        "\n",
        "    model=\"anthropic-sonnet-4.0\",                # multimodal LVM used by LlamaParse\n",
        "\n",
        "    vendor_multimodal_api_key=required_keys[\"ANTHROPIC_API_KEY\"],\n",
        "\n",
        "    api_key=required_keys[\"LLAMAPARSE_API_KEY\"],\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Use the correct method to load the PDF\n",
        "\n",
        "raw_docs = loader.load_data(filename)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"üìÑ Loaded {len(raw_docs)} pages from PDF (multimodal LlamaParse)\")\n",
        "\n",
        "\n",
        "\n",
        "# Convert to LlamaIndex Documents\n",
        "\n",
        "# NOTE: LlamaParse documents expose `.text`; keep a fallback to `.page_content` for safety.\n",
        "\n",
        "llama_documents: List[LlamaIndexDocument] = []\n",
        "\n",
        "for i, doc in enumerate(raw_docs):\n",
        "\n",
        "    text = getattr(doc, \"text\", None)\n",
        "\n",
        "    if text is None:\n",
        "\n",
        "        text = getattr(doc, \"page_content\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "    metadata = getattr(doc, \"metadata\", {}) or {}\n",
        "\n",
        "    metadata = {\n",
        "\n",
        "        **metadata,\n",
        "\n",
        "        \"source\": filename,\n",
        "\n",
        "        \"page\": i + 1,\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    llama_documents.append(\n",
        "\n",
        "        LlamaIndexDocument(\n",
        "\n",
        "            text=text,\n",
        "\n",
        "            metadata=metadata,\n",
        "\n",
        "        )\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Converted {len(llama_documents)} pages to LlamaIndex format\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Chunk Documents into Nodes\n",
        "\n",
        "**Instructions**: Split documents into smaller chunks for better retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 312 nodes from 307 documents\n",
            "üìä Chunk size: 1024 tokens, Overlap: 200 tokens\n",
            "üìù Formatting preserved: separators kept, paragraphs maintained\n",
            "üè∑Ô∏è  Chunk metadata enriched: chunk_index and chunk_id added\n"
          ]
        }
      ],
      "source": [
        "# Create nodes using SentenceSplitter with formatting preservation\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "parser = SentenceSplitter(\n",
        "    chunk_size=1024,          # token-based chunk size\n",
        "    chunk_overlap=200,\n",
        "    # use blank lines as paragraph/markdown block separators\n",
        "    paragraph_separator=\"\\n\\n\\n\",\n",
        ")\n",
        "nodes = parser.get_nodes_from_documents(llama_documents)\n",
        "\n",
        "# Enrich nodes with chunk-level metadata for downstream analysis / tooling\n",
        "for i, node in enumerate(nodes):\n",
        "    if node.metadata is None:\n",
        "        node.metadata = {}\n",
        "    \n",
        "    source = node.metadata.get(\"source\", \"allplan_docs_collection\")\n",
        "    page = node.metadata.get(\"page\", \"NA\")\n",
        "    \n",
        "    node.metadata[\"chunk_index\"] = i\n",
        "    node.metadata[\"chunk_id\"] = f\"{source}_p{page}_c{i}\"\n",
        "\n",
        "print(f\"‚úÖ Created {len(nodes)} nodes from {len(llama_documents)} documents\")\n",
        "print(f\"üìä Chunk size: 1024 tokens, Overlap: 200 tokens\")\n",
        "print(f\"üìù Formatting preserved: separators kept, paragraphs maintained\")\n",
        "print(f\"üè∑Ô∏è  Chunk metadata enriched: chunk_index and chunk_id added\")\n",
        "\n",
        "# Convert back to LangChain Documents for compatibility\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=doc.text,\n",
        "        metadata=doc.metadata\n",
        "    )\n",
        "    for doc in llama_documents\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Connect to Qdrant and Build Vector Index\n",
        "\n",
        "**Instructions**: Connect to Qdrant vector database and create/update the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ VectorStoreIndex built and documents stored in Qdrant\n",
            "üì¶ Collection: allplan_docs_collection\n",
            "üìê Vector dimension: 2048 (Jina v4)\n"
          ]
        }
      ],
      "source": [
        "# Create Qdrant client\n",
        "client = qdrant_client.QdrantClient(\n",
        "    url=required_keys[\"QDRANT_URL\"],\n",
        "    api_key=required_keys[\"QDRANT_API_KEY\"],\n",
        ")\n",
        "\n",
        "# Create vector store with explicit 2048-dimensional vectors (matches embedding_dim)\n",
        "vector_store = QdrantVectorStore(\n",
        "    collection_name=\"allplan_docs_collection\",\n",
        "    client=client,\n",
        "    # Match the enforced Jina embedding dimensionality (VECTOR_DIM = 2048)\n",
        "    vector_size=embedding_dim,\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Build index\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ VectorStoreIndex built and documents stored in Qdrant\")\n",
        "print(f\"üì¶ Collection: allplan_docs_collection\")\n",
        "print(f\"üìê Vector dimension: {embedding_dim} (Jina v4)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Configure RAG Pipeline\n",
        "\n",
        "**Instructions**: Set up the retriever, LLM, and complete RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAG pipeline fully configured and ready!\n",
            "üìù Retrieval: Top-3 similarity search\n",
            "ü§ñ LLM: Claude 3 Haiku (text-only generation; parsing is multimodal via LlamaParse)\n"
          ]
        }
      ],
      "source": [
        "# Create retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=3,\n",
        ")\n",
        "\n",
        "# Configure LLM (use a real Claude 3 Haiku model ID)\n",
        "chat_model = ChatAnthropic(\n",
        "    model=\"claude-haiku-4-5-20251001\",\n",
        "    temperature=0.1,\n",
        "    max_retries=2,\n",
        "    api_key=required_keys[\"ANTHROPIC_API_KEY\"],\n",
        ")\n",
        "\n",
        "llama_llm = LangChainLLM(llm=chat_model)\n",
        "\n",
        "# Define LlamaIndexRAG class\n",
        "class LlamaIndexRAG(RAG):\n",
        "    \"\"\"RAG implementation using LlamaIndex Core, Jina embeddings, and Qdrant.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm,\n",
        "        documents: List[Document],\n",
        "        k: int = 3,\n",
        "        index: VectorStoreIndex = None,\n",
        "        retriever: VectorIndexRetriever = None,\n",
        "        query_engine: RetrieverQueryEngine = None,\n",
        "    ):\n",
        "        super().__init__(llm, documents, k)\n",
        "        self.index = index\n",
        "        self.retriever = retriever\n",
        "\n",
        "        # Create query engine if not provided\n",
        "        if query_engine is None and self.retriever is not None:\n",
        "            self.query_engine = RetrieverQueryEngine.from_args(\n",
        "                retriever=self.retriever,\n",
        "                llm=llama_llm,\n",
        "                response_mode=ResponseMode.COMPACT,\n",
        "            )\n",
        "        else:\n",
        "            self.query_engine = query_engine\n",
        "\n",
        "    def retrieve(self, question: str, *args: Any, **kwargs: Any) -> List[Document]:\n",
        "        \"\"\"Retrieve relevant documents using LlamaIndex retriever.\"\"\"\n",
        "        if self.retriever is None:\n",
        "            return []\n",
        "\n",
        "        # Retrieve nodes from LlamaIndex\n",
        "        nodes = self.retriever.retrieve(question)\n",
        "\n",
        "        # Convert LlamaIndex nodes to LangChain Documents\n",
        "        langchain_docs = []\n",
        "        for node in nodes[: self.k]:\n",
        "            doc = Document(\n",
        "                page_content=node.get_content(),\n",
        "                metadata=getattr(node, \"metadata\", {}) or {},\n",
        "            )\n",
        "            langchain_docs.append(doc)\n",
        "\n",
        "        return langchain_docs\n",
        "\n",
        "    def generate(self, question: str, context: str, *args: Any, **kwargs: Any) -> str:\n",
        "        \"\"\"Generate answer using LlamaIndex query engine.\"\"\"\n",
        "        if self.query_engine is None:\n",
        "            # Fallback: simple LLM call using raw context\n",
        "            prompt = (\n",
        "                \"Using the following context, answer the question:\\n\\n\"\n",
        "                f\"Context: {context}\\n\\n\"\n",
        "                f\"Question: {question}\\n\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            answer = self.llm.invoke(prompt)\n",
        "            if hasattr(answer, \"content\"):\n",
        "                return answer.content\n",
        "            return str(answer)\n",
        "\n",
        "        # Use LlamaIndex query engine\n",
        "        response = self.query_engine.query(question)\n",
        "        return str(response)\n",
        "\n",
        "\n",
        "# Create RAG instance\n",
        "rag = LlamaIndexRAG(\n",
        "    llm=chat_model,\n",
        "    documents=documents,\n",
        "    k=3,\n",
        "    index=index,\n",
        "    retriever=retriever,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RAG pipeline fully configured and ready!\")\n",
        "print(\"üìù Retrieval: Top-3 similarity search\")\n",
        "print(\"ü§ñ LLM: Claude 3 Haiku (text-only generation; parsing is multimodal via LlamaParse)\")\n",
        "\n",
        "# Create results directory if it doesn't exist\n",
        "results_dir = Path(glob.DATA_PKG_DIR) / \"evaluation_results\"\n",
        "results_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Pipeline with Sample Query\n",
        "\n",
        "**Instructions**: Run this cell to verify the RAG pipeline is working correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Testing pipeline with query: 'What is Allplan?'\n",
            "\n",
            "‚úÖ Pipeline test successful!\n",
            "   Answer length: 335 characters\n",
            "   Retrieved documents: 3\n",
            "\n",
            "üí¨ Sample answer: Allplan 2020 is a high-performance CAD program designed for architects and civil engineers. It provides tools and features to help users carry out common operations and accomplish daily tasks in their...\n"
          ]
        }
      ],
      "source": [
        "# Run a test query to confirm everything works\n",
        "test_query = \"What is Allplan?\"\n",
        "print(f\"\\nüß™ Testing pipeline with query: '{test_query}'\\n\")\n",
        "\n",
        "try:\n",
        "    answer, relevant_docs = rag.answer(question=test_query)\n",
        "    print(\"‚úÖ Pipeline test successful!\")\n",
        "    print(f\"   Answer length: {len(answer)} characters\")\n",
        "    print(f\"   Retrieved documents: {len(relevant_docs)}\")\n",
        "    print(f\"\\nüí¨ Sample answer: {answer[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Pipeline test failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìã Human Evaluation Rubric (Shared by A and B)\n",
        "\n",
        "## Scoring Criteria\n",
        "Both evaluators must use this **exact rubric** to keep evaluations comparable.\n",
        "\n",
        "### 1Ô∏è‚É£ Relevance\n",
        "*Does the answer address the question asked?*\n",
        "\n",
        "- **5 - Excellent**: Directly and fully addresses the question\n",
        "- **4 - Good**: Addresses the question with only minor tangential content\n",
        "- **3 - Fair**: Partially addresses the question but includes irrelevant information\n",
        "- **2 - Poor**: Barely addresses the question, mostly irrelevant\n",
        "- **1 - Very Poor**: Completely off-topic or does not address the question\n",
        "\n",
        "### 2Ô∏è‚É£ Accuracy\n",
        "*Is the information provided factually correct?*\n",
        "\n",
        "- **5 - Excellent**: All information is accurate and correct\n",
        "- **4 - Good**: Mostly accurate with minor errors that don't affect understanding\n",
        "- **3 - Fair**: Some accurate information but notable errors present\n",
        "- **2 - Poor**: Many errors, unreliable information\n",
        "- **1 - Very Poor**: Completely incorrect, misleading, or fabricated\n",
        "\n",
        "### 3Ô∏è‚É£ Completeness\n",
        "*Does the answer cover all important aspects of the question?*\n",
        "\n",
        "- **5 - Excellent**: Comprehensive, covers all key aspects thoroughly\n",
        "- **4 - Good**: Covers most aspects with minor gaps\n",
        "- **3 - Fair**: Covers basic aspects but missing important details\n",
        "- **2 - Poor**: Significant gaps, incomplete answer\n",
        "- **1 - Very Poor**: Severely incomplete, missing most key information\n",
        "\n",
        "### 4Ô∏è‚É£ Source Quality\n",
        "*Are the retrieved documents relevant and helpful?*\n",
        "\n",
        "- **5 - Excellent**: All retrieved documents are highly relevant and support the answer\n",
        "- **4 - Good**: Most documents are relevant and helpful\n",
        "- **3 - Fair**: Some relevant documents but also irrelevant ones\n",
        "- **2 - Poor**: Few relevant documents, mostly irrelevant\n",
        "- **1 - Very Poor**: No relevant documents retrieved\n",
        "\n",
        "## Overall Score\n",
        "- Computed as the **average** of the four criteria above\n",
        "- Evaluators may slightly adjust if needed, but should explain why in notes\n",
        "\n",
        "## Notes Fields\n",
        "For each query, provide:\n",
        "- **Brief notes per criterion** (what worked, what didn't)\n",
        "- **General comments** (overall impression, suggestions, concerns)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üë§ SECTION 2 ‚Äî EVALUATOR A\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANT - READ CAREFULLY\n",
        "\n",
        "### Audience\n",
        "- This section is **exclusively for Evaluator A**\n",
        "- **Do NOT scroll to Evaluator B's section or comparison sections**\n",
        "\n",
        "### Estimated Time\n",
        "- 20-30 minutes (depending on number of queries)\n",
        "\n",
        "### Your Task\n",
        "1. You will see a list of queries (questions to test the RAG system)\n",
        "2. For each query:\n",
        "   - The system will run the RAG pipeline and show:\n",
        "     - üìù The query text\n",
        "     - üí¨ The model's answer\n",
        "     - üìö Retrieved context snippets\n",
        "     - üîó Information about the sources\n",
        "3. Your job is to:\n",
        "   - Read the question, answer, and context carefully\n",
        "   - Score the answer on **4 criteria** (1-5 each) using the rubric above\n",
        "   - Provide short notes for each criterion\n",
        "   - Provide general comments for the query\n",
        "\n",
        "### Workflow\n",
        "1. **Start a new session** (runs automatically)\n",
        "2. **Define your queries** (or use provided list)\n",
        "3. **Evaluate each query** (fill in scores and notes)\n",
        "4. **Save results** (happens automatically)\n",
        "5. **STOP** - Do not proceed to other sections\n",
        "\n",
        "### Checkpoint\n",
        "You are done when:\n",
        "- ‚úÖ All queries are evaluated\n",
        "- ‚úÖ The save step confirms results were written\n",
        "- ‚úÖ You see your results file path\n",
        "\n",
        "**Then STOP and do not proceed to Evaluator B or comparison sections!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator A - Session Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate unique session ID for Evaluator A\n",
        "evaluator_a_session_id = f\"evaluator_a_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}\"\n",
        "print(f\"üÜî Evaluator A Session ID: {evaluator_a_session_id}\")\n",
        "\n",
        "# Define queries for Evaluator A\n",
        "# NOTE: These should be the same queries that Evaluator B will evaluate\n",
        "evaluator_a_queries = [\n",
        "    \"How do I create a new project in Allplan?\",\n",
        "    \"What are the system requirements for Allplan 2020?\",\n",
        "    \"How can I export drawings to PDF?\",\n",
        "    \"What is the purpose of layers in Allplan?\",\n",
        "    \"How do I import CAD files into Allplan?\",\n",
        "]\n",
        "\n",
        "print(f\"\\nüìù Evaluator A will evaluate {len(evaluator_a_queries)} queries:\")\n",
        "for i, q in enumerate(evaluator_a_queries, 1):\n",
        "    print(f\"   {i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to Collect Evaluation Scores\n",
        "\n",
        "**Note**: In an interactive notebook, this would collect user input. For demonstration, it shows the structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to collect evaluation scores from user\n",
        "def collect_evaluation(query_idx: int, query: str, answer: str, docs: List[Document]) -> Dict:\n",
        "    \"\"\"\n",
        "    Display query/answer/context and collect evaluation scores.\n",
        "    In an interactive notebook, this would use input() or a form.\n",
        "    For this template, we'll show what needs to be collected.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"üìù QUERY {query_idx}/{len(evaluator_a_queries)}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\n‚ùì QUESTION:\\n{query}\")\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"\\nüí¨ ANSWER:\\n{answer}\")\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"\\nüìö RETRIEVED CONTEXT ({len(docs)} documents):\\n\")\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"[{i}] Page {doc.metadata.get('page', 'N/A')}:\")\n",
        "        print(f\"    {doc.page_content[:300]}...\")\n",
        "        print()\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üìä EVALUATION TIME - Use the rubric above (1-5 scale)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # In a real interactive session, you would use input() here\n",
        "    # For this template, we'll structure what needs to be collected\n",
        "\n",
        "    evaluation = {\n",
        "        \"query_index\": query_idx,\n",
        "        \"query\": query,\n",
        "        \"answer\": answer,\n",
        "        \"retrieved_docs_count\": len(docs),\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "\n",
        "        # These would be collected via input() in interactive mode\n",
        "        \"scores\": {\n",
        "            \"relevance\": None,  # input(\"Relevance (1-5): \")\n",
        "            \"accuracy\": None,   # input(\"Accuracy (1-5): \")\n",
        "            \"completeness\": None,  # input(\"Completeness (1-5): \")\n",
        "            \"source_quality\": None,  # input(\"Source Quality (1-5): \")\n",
        "        },\n",
        "        \"notes\": {\n",
        "            \"relevance_notes\": None,  # input(\"Relevance notes: \")\n",
        "            \"accuracy_notes\": None,   # input(\"Accuracy notes: \")\n",
        "            \"completeness_notes\": None,  # input(\"Completeness notes: \")\n",
        "            \"source_quality_notes\": None,  # input(\"Source quality notes: \")\n",
        "        },\n",
        "        \"general_comments\": None,  # input(\"General comments: \")\n",
        "    }\n",
        "\n",
        "    # Calculate overall score (average of 4 criteria)\n",
        "    scores = evaluation[\"scores\"]\n",
        "    if all(v is not None for v in scores.values()):\n",
        "        evaluation[\"overall_score\"] = sum(scores.values()) / len(scores)\n",
        "\n",
        "    print(\"\\n‚ö†Ô∏è IN INTERACTIVE MODE: You would fill in scores and notes here\")\n",
        "    print(\"   For now, this is a template structure showing what to collect\")\n",
        "\n",
        "    return evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator A - Interactive Evaluation Loop\n",
        "\n",
        "**Instructions**:\n",
        "- Run the cell below to start the evaluation loop\n",
        "- For each query, you will:\n",
        "  1. See the question, answer, and context\n",
        "  2. Enter scores (1-5) for each criterion\n",
        "  3. Provide notes and comments\n",
        "- Results are automatically saved after each query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EVALUATOR A: Main evaluation loop\n",
        "print(\"=\"*80)\n",
        "print(\"üöÄ STARTING EVALUATOR A EVALUATION SESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "evaluator_a_results = []\n",
        "\n",
        "for idx, query in enumerate(evaluator_a_queries, 1):\n",
        "    try:\n",
        "        # Run RAG pipeline\n",
        "        answer, relevant_docs = rag.answer(question=query)\n",
        "\n",
        "        # Collect evaluation (in interactive mode, this would prompt for input)\n",
        "        evaluation = collect_evaluation(idx, query, answer, relevant_docs)\n",
        "\n",
        "        # Store result\n",
        "        evaluator_a_results.append(evaluation)\n",
        "\n",
        "        print(f\"\\n‚úÖ Query {idx} evaluation recorded\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluating query {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"‚úÖ EVALUATOR A COMPLETED {len(evaluator_a_results)} EVALUATIONS\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Evaluator A results\n",
        "evaluator_a_file = results_dir / f\"{evaluator_a_session_id}.json\"\n",
        "\n",
        "with open(evaluator_a_file, 'w') as f:\n",
        "    json.dump({\n",
        "        \"session_id\": evaluator_a_session_id,\n",
        "        \"evaluator\": \"A\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"num_queries\": len(evaluator_a_queries),\n",
        "        \"evaluations\": evaluator_a_results,\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Evaluator A results saved to:\")\n",
        "print(f\"   {evaluator_a_file}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ EVALUATOR A: YOU ARE DONE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Do NOT proceed to other sections!\")\n",
        "print(\"   Please close this notebook now or wait for the organizer.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üë§ SECTION 3 ‚Äî EVALUATOR B\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANT - READ CAREFULLY\n",
        "\n",
        "### Audience\n",
        "- This section is **exclusively for Evaluator B**\n",
        "- **Do NOT scroll up to Evaluator A's section**\n",
        "\n",
        "### Independence & Blindness\n",
        "- **You should NOT see Evaluator A's queries, answers, or scores in advance**\n",
        "- **Use the same rubric** but rely on your own judgment\n",
        "- **Until both evaluators are finished**, no one should open comparison sections\n",
        "\n",
        "### Estimated Time\n",
        "- 20-30 minutes (depending on number of queries)\n",
        "\n",
        "### Your Task\n",
        "1. You will see a list of queries (the same queries as Evaluator A)\n",
        "2. For each query:\n",
        "   - The system will run the RAG pipeline and show:\n",
        "     - üìù The query text\n",
        "     - üí¨ The model's answer\n",
        "     - üìö Retrieved context snippets\n",
        "     - üîó Information about the sources\n",
        "3. Your job is to:\n",
        "   - Read the question, answer, and context carefully\n",
        "   - Score the answer on **4 criteria** (1-5 each) using the rubric above\n",
        "   - Provide short notes for each criterion\n",
        "   - Provide general comments for the query\n",
        "\n",
        "### Workflow\n",
        "1. **Start a new session** (runs automatically)\n",
        "2. **Use provided queries** (same as Evaluator A)\n",
        "3. **Evaluate each query** (fill in scores and notes)\n",
        "4. **Save results** (happens automatically)\n",
        "5. **STOP** - Do not proceed to comparison sections\n",
        "\n",
        "### Checkpoint\n",
        "You are done when:\n",
        "- ‚úÖ All queries are evaluated\n",
        "- ‚úÖ The save step confirms results were written\n",
        "- ‚úÖ You see your results file path\n",
        "\n",
        "**Then STOP - Do not open comparison or automated evaluation results yet!**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator B - Session Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate unique session ID for Evaluator B\n",
        "evaluator_b_session_id = f\"evaluator_b_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}\"\n",
        "print(f\"üÜî Evaluator B Session ID: {evaluator_b_session_id}\")\n",
        "\n",
        "# Define queries for Evaluator B (SAME as Evaluator A for comparison)\n",
        "evaluator_b_queries = [\n",
        "    \"How do I create a new project in Allplan?\",\n",
        "    \"What are the system requirements for Allplan 2020?\",\n",
        "    \"How can I export drawings to PDF?\",\n",
        "    \"What is the purpose of layers in Allplan?\",\n",
        "    \"How do I import CAD files into Allplan?\",\n",
        "]\n",
        "\n",
        "print(f\"\\nüìù Evaluator B will evaluate {len(evaluator_b_queries)} queries:\")\n",
        "for i, q in enumerate(evaluator_b_queries, 1):\n",
        "    print(f\"   {i}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator B - Interactive Evaluation Loop\n",
        "\n",
        "**Instructions**:\n",
        "- Run the cell below to start the evaluation loop\n",
        "- For each query, you will:\n",
        "  1. See the question, answer, and context\n",
        "  2. Enter scores (1-5) for each criterion\n",
        "  3. Provide notes and comments\n",
        "- Results are automatically saved after each query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EVALUATOR B: Main evaluation loop\n",
        "print(\"=\"*80)\n",
        "print(\"üöÄ STARTING EVALUATOR B EVALUATION SESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "evaluator_b_results = []\n",
        "\n",
        "for idx, query in enumerate(evaluator_b_queries, 1):\n",
        "    try:\n",
        "        # Run RAG pipeline\n",
        "        answer, relevant_docs = rag.answer(question=query)\n",
        "\n",
        "        # Collect evaluation (in interactive mode, this would prompt for input)\n",
        "        evaluation = collect_evaluation(idx, query, answer, relevant_docs)\n",
        "\n",
        "        # Store result\n",
        "        evaluator_b_results.append(evaluation)\n",
        "\n",
        "        print(f\"\\n‚úÖ Query {idx} evaluation recorded\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluating query {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"‚úÖ EVALUATOR B COMPLETED {len(evaluator_b_results)} EVALUATIONS\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Evaluator B results\n",
        "evaluator_b_file = results_dir / f\"{evaluator_b_session_id}.json\"\n",
        "\n",
        "with open(evaluator_b_file, 'w') as f:\n",
        "    json.dump({\n",
        "        \"session_id\": evaluator_b_session_id,\n",
        "        \"evaluator\": \"B\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"num_queries\": len(evaluator_b_queries),\n",
        "        \"evaluations\": evaluator_b_results,\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Evaluator B results saved to:\")\n",
        "print(f\"   {evaluator_b_file}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ EVALUATOR B: YOU ARE DONE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Do NOT proceed to comparison sections!\")\n",
        "print(\"   Please close this notebook now or wait for the organizer.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluator B - Observations and Notes\n",
        "\n",
        "**Instructions**: Document your observations below:\n",
        "\n",
        "#### Query 1: [Query text]\n",
        "- **Relevance** (1-5): \n",
        "- **Accuracy** (1-5): \n",
        "- **Completeness** (1-5): \n",
        "- **Retrieved Docs Quality** (1-5): \n",
        "- **Notes**: \n",
        "\n",
        "#### Query 2: [Query text]\n",
        "- **Relevance** (1-5): \n",
        "- **Accuracy** (1-5): \n",
        "- **Completeness** (1-5): \n",
        "- **Retrieved Docs Quality** (1-5): \n",
        "- **Notes**: \n",
        "\n",
        "#### Query 3: [Query text]\n",
        "- **Relevance** (1-5): \n",
        "- **Accuracy** (1-5): \n",
        "- **Completeness** (1-5): \n",
        "- **Retrieved Docs Quality** (1-5): \n",
        "- **Notes**: \n",
        "\n",
        "#### Overall Observations:\n",
        "- **Strengths**: \n",
        "- **Weaknesses**: \n",
        "- **Suggestions**: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìä Automated Evaluation with DeepEval\n",
        "\n",
        "## Instructions:\n",
        "Now that both evaluators have completed their manual testing, let's run automated metrics on the ground truth dataset.\n",
        "\n",
        "This will evaluate the RAG system on:\n",
        "- **Answer Relevancy**: How relevant is the answer to the question?\n",
        "- **Faithfulness**: Is the answer grounded in the retrieved context?\n",
        "- **Contextual Relevancy**: Are retrieved documents relevant?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Ground Truth QA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load annotated evaluation data\n",
        "json = JSONService(\n",
        "    path=\"generated_qa_data_tum.json\",\n",
        "    root_path=glob.DATA_PKG_DIR,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "qa_data = json.doRead()\n",
        "print(f\"‚úÖ Loaded {len(qa_data)} evaluation samples\")\n",
        "\n",
        "# Extract components\n",
        "ground_truth_contexts = [item[\"context\"] for item in qa_data]\n",
        "sample_queries = [item[\"question\"] for item in qa_data]\n",
        "expected_responses = [item[\"answer\"] for item in qa_data]\n",
        "\n",
        "print(f\"\\nüìã Dataset composition:\")\n",
        "print(f\"   - Questions: {len(sample_queries)}\")\n",
        "print(f\"   - Contexts: {len(ground_truth_contexts)}\")\n",
        "print(f\"   - Expected answers: {len(expected_responses)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Evaluation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation dataset builder\n",
        "builder = eval_builder.EvalDatasetBuilder(rag)\n",
        "\n",
        "# Build the evaluation dataset\n",
        "evaluation_dataset = builder.build_evaluation_dataset(\n",
        "    input_contexts=ground_truth_contexts,\n",
        "    sample_queries=sample_queries,\n",
        "    expected_responses=expected_responses,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Evaluation dataset built with {len(evaluation_dataset.test_cases)} test cases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run DeepEval Scoring\n",
        "\n",
        "**Note**: This may take several minutes depending on dataset size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize scorer\n",
        "scorer = deep.DeepEvalScorer(evaluation_dataset)\n",
        "\n",
        "# Calculate scores\n",
        "print(\"üîÑ Running DeepEval metrics... (this may take a few minutes)\")\n",
        "results = scorer.calculate_scores()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä DEEPEVAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Overall Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get overall metrics summary\n",
        "overall_metrics = scorer.get_overall_metrics()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà OVERALL METRICS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(overall_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate and Save Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary\n",
        "summary = scorer.get_summary(save_to_file=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù EVALUATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(summary)\n",
        "print(\"\\n‚úÖ Summary saved to file!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîç SECTION 5 ‚Äî Comparison & Discussion\n",
        "\n",
        "## Audience\n",
        "- **Organizer plus both evaluators jointly**, after both have finished\n",
        "\n",
        "## Overview\n",
        "This section loads:\n",
        "- Evaluator A's results\n",
        "- Evaluator B's results\n",
        "- Optional automated metrics\n",
        "\n",
        "It then:\n",
        "- Aligns evaluations by query\n",
        "- Compares individual scores (relevance, accuracy, completeness, source quality)\n",
        "- Compares overall scores\n",
        "\n",
        "## Graceful Handling of Missing Data\n",
        "- If one evaluator did not complete all queries, the notebook will:\n",
        "  - Compare only on the overlapping queries\n",
        "  - Warn when data is missing or incomplete\n",
        "\n",
        "## What You'll See\n",
        "- **Aggregate statistics**: Average scores per evaluator, per criterion\n",
        "- **Inter-rater agreement**: Measure of disagreement between evaluators\n",
        "- **Query-level comparison**: Side-by-side scores for each query\n",
        "- **Discussion prompts**: Questions to guide your analysis\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load most recent evaluator results\n",
        "def load_evaluator_results(evaluator: str) -> Dict:\n",
        "    \"\"\"Load the most recent results for a given evaluator.\"\"\"\n",
        "    pattern = f\"evaluator_{evaluator.lower()}_*.json\"\n",
        "    files = sorted(results_dir.glob(pattern),\n",
        "                   key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "\n",
        "    if not files:\n",
        "        print(f\"‚ö†Ô∏è  No results found for Evaluator {evaluator}\")\n",
        "        return None\n",
        "\n",
        "    with open(files[0], 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"‚úÖ Loaded Evaluator {evaluator} results from: {files[0].name}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "# Load results\n",
        "eval_a_data = load_evaluator_results(\"A\")\n",
        "eval_b_data = load_evaluator_results(\"B\")\n",
        "\n",
        "if eval_a_data is None or eval_b_data is None:\n",
        "    print(\"\\n‚ùå Cannot proceed with comparison - missing evaluator data\")\n",
        "else:\n",
        "    print(f\"\\nüìä Comparison ready:\")\n",
        "    print(f\"   - Evaluator A: {len(eval_a_data['evaluations'])} evaluations\")\n",
        "    print(f\"   - Evaluator B: {len(eval_b_data['evaluations'])} evaluations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aggregate Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_aggregate_stats(evaluations: List[Dict]) -> Dict:\n",
        "    \"\"\"Calculate aggregate statistics for an evaluator's results.\"\"\"\n",
        "\n",
        "    # Filter out evaluations with None scores\n",
        "    valid_evals = [e for e in evaluations if all(\n",
        "        s is not None for s in e['scores'].values())]\n",
        "\n",
        "    if not valid_evals:\n",
        "        return {\n",
        "            \"num_evaluations\": 0,\n",
        "            \"avg_relevance\": None,\n",
        "            \"avg_accuracy\": None,\n",
        "            \"avg_completeness\": None,\n",
        "            \"avg_source_quality\": None,\n",
        "            \"avg_overall\": None,\n",
        "        }\n",
        "\n",
        "    n = len(valid_evals)\n",
        "\n",
        "    return {\n",
        "        \"num_evaluations\": n,\n",
        "        \"avg_relevance\": sum(e['scores']['relevance'] for e in valid_evals) / n,\n",
        "        \"avg_accuracy\": sum(e['scores']['accuracy'] for e in valid_evals) / n,\n",
        "        \"avg_completeness\": sum(e['scores']['completeness'] for e in valid_evals) / n,\n",
        "        \"avg_source_quality\": sum(e['scores']['source_quality'] for e in valid_evals) / n,\n",
        "        \"avg_overall\": sum(e.get('overall_score', 0) for e in valid_evals) / n,\n",
        "    }\n",
        "\n",
        "\n",
        "if eval_a_data and eval_b_data:\n",
        "    stats_a = calculate_aggregate_stats(eval_a_data['evaluations'])\n",
        "    stats_b = calculate_aggregate_stats(eval_b_data['evaluations'])\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üìà AGGREGATE STATISTICS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nüë§ Evaluator A:\")\n",
        "    for key, value in stats_a.items():\n",
        "        if value is not None and key != \"num_evaluations\":\n",
        "            print(\n",
        "                f\"   {key.replace('avg_', '').replace('_', ' ').title()}: {value:.2f}\")\n",
        "\n",
        "    print(\"\\nüë§ Evaluator B:\")\n",
        "    for key, value in stats_b.items():\n",
        "        if value is not None and key != \"num_evaluations\":\n",
        "            print(\n",
        "                f\"   {key.replace('avg_', '').replace('_', ' ').title()}: {value:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inter-Rater Agreement Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_agreement(eval_a: List[Dict], eval_b: List[Dict]) -> Dict:\n",
        "    \"\"\"Calculate inter-rater agreement metrics.\"\"\"\n",
        "\n",
        "    # Match evaluations by query\n",
        "    agreements = []\n",
        "\n",
        "    for ea, eb in zip(eval_a, eval_b):\n",
        "        if ea['query'] != eb['query']:\n",
        "            print(f\"‚ö†Ô∏è  Query mismatch: '{ea['query']}' vs '{eb['query']}'\")\n",
        "            continue\n",
        "\n",
        "        if all(s is not None for s in ea['scores'].values()) and all(s is not None for s in eb['scores'].values()):\n",
        "            diff_relevance = abs(\n",
        "                ea['scores']['relevance'] - eb['scores']['relevance'])\n",
        "            diff_accuracy = abs(\n",
        "                ea['scores']['accuracy'] - eb['scores']['accuracy'])\n",
        "            diff_completeness = abs(\n",
        "                ea['scores']['completeness'] - eb['scores']['completeness'])\n",
        "            diff_source = abs(ea['scores']['source_quality'] -\n",
        "                              eb['scores']['source_quality'])\n",
        "            diff_overall = abs(ea.get('overall_score', 0) -\n",
        "                               eb.get('overall_score', 0))\n",
        "\n",
        "            agreements.append({\n",
        "                'query': ea['query'],\n",
        "                'diff_relevance': diff_relevance,\n",
        "                'diff_accuracy': diff_accuracy,\n",
        "                'diff_completeness': diff_completeness,\n",
        "                'diff_source': diff_source,\n",
        "                'diff_overall': diff_overall,\n",
        "            })\n",
        "\n",
        "    if not agreements:\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        'num_queries': len(agreements),\n",
        "        'mean_abs_diff_overall': sum(a['diff_overall'] for a in agreements) / len(agreements),\n",
        "        'max_diff_overall': max(a['diff_overall'] for a in agreements),\n",
        "        'queries_with_large_disagreement': [a for a in agreements if a['diff_overall'] >= 2.0],\n",
        "    }\n",
        "\n",
        "\n",
        "if eval_a_data and eval_b_data:\n",
        "    agreement = calculate_agreement(\n",
        "        eval_a_data['evaluations'], eval_b_data['evaluations'])\n",
        "\n",
        "    if agreement:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ü§ù INTER-RATER AGREEMENT\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nQueries compared: {agreement['num_queries']}\")\n",
        "        print(\n",
        "            f\"Mean absolute difference (overall score): {agreement['mean_abs_diff_overall']:.2f}\")\n",
        "        print(\n",
        "            f\"Maximum difference (overall score): {agreement['max_diff_overall']:.2f}\")\n",
        "\n",
        "        if agreement['queries_with_large_disagreement']:\n",
        "            print(f\"\\n‚ö†Ô∏è  Queries with large disagreement (‚â•2.0 points):\")\n",
        "            for q in agreement['queries_with_large_disagreement']:\n",
        "                print(\n",
        "                    f\"   - '{q['query'][:60]}...' (diff: {q['diff_overall']:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üì§ SECTION 6 ‚Äî Export & Reporting\n",
        "\n",
        "## Purpose\n",
        "Export all evaluation data for further analysis or sharing with stakeholders.\n",
        "\n",
        "## What Can Be Exported\n",
        "- **Human evaluation data** (from Evaluator A and B) ‚Üí CSV files\n",
        "- **Automated metrics** (if present) ‚Üí CSV file\n",
        "- **Combined comparison report** ‚Üí CSV or JSON\n",
        "- **Entire notebook** ‚Üí HTML or PDF (via Jupyter tools)\n",
        "\n",
        "## How to Export\n",
        "\n",
        "### CSV Export\n",
        "- Run the cells below to generate CSV files\n",
        "- Files will be saved in the `evaluation_results` directory\n",
        "\n",
        "### Notebook Export\n",
        "- Use Jupyter's File ‚Üí Download as ‚Üí HTML/PDF\n",
        "- For PDF export, you may need LaTeX installed\n",
        "- Alternative: Export to HTML and print to PDF from browser\n",
        "\n",
        "## Versioning\n",
        "- **Recommendation**: Track the notebook file and exported CSVs in git\n",
        "- The notebook contains a version string for tracking changes over time\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_evaluations_to_csv(eval_data: Dict, output_file: Path):\n",
        "    \"\"\"Export evaluation results to CSV format.\"\"\"\n",
        "\n",
        "    rows = []\n",
        "    for e in eval_data['evaluations']:\n",
        "        row = {\n",
        "            'query_index': e['query_index'],\n",
        "            'query': e['query'],\n",
        "            'timestamp': e['timestamp'],\n",
        "            'relevance': e['scores'].get('relevance'),\n",
        "            'accuracy': e['scores'].get('accuracy'),\n",
        "            'completeness': e['scores'].get('completeness'),\n",
        "            'source_quality': e['scores'].get('source_quality'),\n",
        "            'overall_score': e.get('overall_score'),\n",
        "            'relevance_notes': e['notes'].get('relevance_notes'),\n",
        "            'accuracy_notes': e['notes'].get('accuracy_notes'),\n",
        "            'completeness_notes': e['notes'].get('completeness_notes'),\n",
        "            'source_quality_notes': e['notes'].get('source_quality_notes'),\n",
        "            'general_comments': e.get('general_comments'),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"‚úÖ Exported to: {output_file}\")\n",
        "\n",
        "\n",
        "# Export Evaluator A and B results to CSV\n",
        "if eval_a_data:\n",
        "    export_file_a = results_dir / f\"{eval_a_data['session_id']}.csv\"\n",
        "    export_evaluations_to_csv(eval_a_data, export_file_a)\n",
        "\n",
        "if eval_b_data:\n",
        "    export_file_b = results_dir / f\"{eval_b_data['session_id']}.csv\"\n",
        "    export_evaluations_to_csv(eval_b_data, export_file_b)\n",
        "\n",
        "print(\"\\nüíæ All evaluation results exported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ‚úÖ Evaluation Complete!\n",
        "\n",
        "## Summary\n",
        "\n",
        "You have successfully:\n",
        "1. ‚úÖ Set up the RAG pipeline with LlamaIndex, Jina AI v4, and Qdrant\n",
        "2. ‚úÖ Conducted independent manual evaluations (Evaluator A & B)\n",
        "3. ‚úÖ Run automated metrics with DeepEval\n",
        "4. ‚úÖ Compared and analyzed results\n",
        "5. ‚úÖ Exported data for further analysis\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- üìä Review the generated summary report\n",
        "- üîß Implement suggested improvements\n",
        "- üîÑ Re-run evaluation to measure improvements\n",
        "- üß™ Consider testing with different:\n",
        "  - Chunk sizes (current: 1024 tokens)\n",
        "  - Retrieval parameters (current: top-3)\n",
        "  - LLM models (current: Claude Haiku 4.5)\n",
        "  - Embedding models (current: Jina v4)\n",
        "\n",
        "## Version\n",
        "- **Notebook Version**: 2.0.0\n",
        "- **Created**: November 22, 2025\n",
        "- **Last Modified**: November 22, 2025\n",
        "- **Key Improvements**: Enhanced evaluation workflow, Jina v4 embeddings, robust comparison analytics\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Notes for Future Iterations\n",
        "\n",
        "- Consider adding more diverse query types\n",
        "- Implement automatic query generation\n",
        "- Add visualization of score distributions\n",
        "- Create dashboards for real-time evaluation monitoring\n",
        "- Integrate with CI/CD pipeline for continuous evaluation\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion Section\n",
        "\n",
        "**Instructions**: Both evaluators should now discuss their findings together.\n",
        "\n",
        "#### Key Discussion Points:\n",
        "\n",
        "1. **What types of queries worked well?**\n",
        "   - \n",
        "\n",
        "2. **What types of queries struggled?**\n",
        "   - \n",
        "\n",
        "3. **Did manual evaluation align with automated metrics?**\n",
        "   - \n",
        "\n",
        "4. **What are the main strengths of this RAG system?**\n",
        "   - \n",
        "\n",
        "5. **What are the main weaknesses?**\n",
        "   - \n",
        "\n",
        "6. **Recommended improvements:**\n",
        "   - Retrieval:\n",
        "   - Generation:\n",
        "   - Chunking strategy:\n",
        "   - Other:\n",
        "\n",
        "7. **Overall assessment (1-10):**\n",
        "   - Evaluator A score: \n",
        "   - Evaluator B score: \n",
        "   - Automated score (average): \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ‚úÖ Evaluation Complete!\n",
        "\n",
        "## Summary\n",
        "\n",
        "You have successfully:\n",
        "1. ‚úÖ Set up the RAG pipeline with LlamaIndex, Jina AI, and Qdrant\n",
        "2. ‚úÖ Conducted independent manual evaluations (Evaluator A & B)\n",
        "3. ‚úÖ Run automated metrics with DeepEval\n",
        "4. ‚úÖ Compared and analyzed results\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Review the generated summary report\n",
        "- Implement suggested improvements\n",
        "- Re-run evaluation to measure improvements\n",
        "- Consider testing with different:\n",
        "  - Chunk sizes\n",
        "  - Retrieval parameters (top-k)\n",
        "  - LLM models\n",
        "  - Embedding models\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
