{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune embeddings using LLM-generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from llm_eval.resources import prompts as pr\n",
    "from llm_eval.resources.rag import generate_qa_couples, eval_qa_couples, answer_with_rag\n",
    "from llm_eval.config import global_config as glob\n",
    "from llm_eval.config.config import model_list\n",
    "# from importlib import reload\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gemini 1.5 Flash': 'gemini-1.5-flash',\n",
      " 'Gemini 1.5 Pro': 'gemini-1.5-pro',\n",
      " 'Gemini 2.0 Flash': 'gemini-2.0-flash-001',\n",
      " 'Gemini 2.0 Flash Thinking': 'gemini-2.0-flash-thinking-exp-01-21',\n",
      " 'Gemini 2.0 Flash-Lite': 'gemini-2.0-flash-lite-preview-02-05',\n",
      " 'Gemini 2.0 Pro': 'gemini-2.0-pro-exp-02-05'}\n",
      "{\"Meta's Llama 3.2 3B\": 'llama3.2',\n",
      " 'Mistral 7B': 'mistral:7b'}\n"
     ]
    }
   ],
   "source": [
    "# Chat models available\n",
    "google_chat = model_list[\"chat_model\"].get(\"google\")\n",
    "ollama_chat = model_list[\"chat_model\"].get(\"ollama\")\n",
    "\n",
    "PrettyPrinter(width=10).pprint(google_chat)\n",
    "PrettyPrinter(width=10).pprint(ollama_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"20240731_Nemetschek SE_Mitarbeiterhandbuch_Employee Handbook.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(f\"{glob.DATA_PKG_DIR}/{filename}\")\n",
    "\n",
    "raw_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "\n",
    "docs_processed = text_splitter.split_documents(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM annotated validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your task is to write a factoid question and an answer based on the given context.\n",
      "The factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
      "Formulate the factoid question in the same style as questions users might ask in a search engine.\n",
      "Do NOT include phrases like \"according to the passage\" or \"context\" in your question.\n",
      "\n",
      "Provide your output in the following format:\n",
      "\n",
      "Output:::\n",
      "Factoid question: (your factoid question)\n",
      "Answer: (your answer to the factoid question; Maximum 300 characters long)\n",
      "\n",
      "Here is the context:\n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Output:::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct QA generation prompt\n",
    "prompt = PromptTemplate(template=pr.QA_generation_prompt, input_variables=[\"context\"])\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50 QA couples...\n",
      "Sampling with replacement\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f549d696aa54845ad02a1711a820655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = ChatOllama(model=ollama_chat[list(ollama_chat.keys())[0]], temperature=0.5)\n",
    "\n",
    "n_generations=50\n",
    "\n",
    "# Generate question/answer pairs\n",
    "outputs = generate_qa_couples(docs_processed, prompt, llm, n_generations, with_replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question groundedness prompt:\n",
      "\n",
      "You will be given a context and a question.\n",
      "Your task is to provide a 'total rating' or scoring how well one can answer the given question based on the given context.\n",
      "Rate your answer on an integer scale of 1 to 5, based on the following score rubric:\n",
      "\n",
      "### Score Rubric:\n",
      "Score 1: The question is not answerable at all given the context. The context does not provide any relevant information to answer the question.\n",
      "Score 2: The question is barely answerable given the context. The context provides minimal relevant information, but the answer would be mostly incorrect, inaccurate, or not factual.\n",
      "Score 3: The question is somewhat answerable given the context. The context provides some relevant information, but the answer would be partially correct, accurate, and factual.\n",
      "Score 4: The question is mostly answerable given the context. The context provides sufficient relevant information, and the answer would be mostly correct, accurate, and factual.\n",
      "Score 5: The question is clearly and unambiguously answerable given the context. The context provides all the necessary relevant information, and the answer would be completely correct, accurate, and factual.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (explain your reasoning for the rating, citing specific examples of how the context does or does not support the question)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "Please always come up with an 'Evaluation:' and 'Total rating:' in your response. \n",
      "In case you are not able to provide a rating, please provide a reason why and use the rating 1.\n",
      "\n",
      "Now here are the question and context.\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:::\n",
      "\n",
      "Question relevancy prompt:\n",
      "\n",
      "You will be given a question.\n",
      "Your task is to provide a 'total rating' representing how useful this question can be to employees at the Nemetschek Group SE.\n",
      "Rate the usefulness of the question on an integer scale of 1 to 5, based on the following score rubric:\n",
      "\n",
      "### Score Rubric:\n",
      "Score 1: The question is not useful at all. It does not provide any relevant information or value to the employees.\n",
      "Score 2: The question is barely useful. It provides minimal relevant information, but its value to the employees is limited.\n",
      "Score 3: The question is somewhat useful. It provides some relevant information, but its value to the employees is moderate.\n",
      "Score 4: The question is mostly useful. It provides sufficient relevant information and is valuable to the employees.\n",
      "Score 5: The question is extremely useful. It provides all the necessary relevant information and is highly valuable to the employees.\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Answer:::\n",
      "Evaluation: (explain your reasoning for the rating, citing specific examples of how the question is or is not useful to the employees)\n",
      "Total rating: (your rating, as a number between 1 and 5)\n",
      "\n",
      "Please always come up with an 'Evaluation:' and 'Total rating:' in your response. \n",
      "In case you are not able to provide a rating, please provide a reason why and use the rating 1.\n",
      "\n",
      "Now here is the question.\n",
      "\n",
      "Question: {question}\n",
      "\n",
      "Answer:::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question-focused prompts:\n",
    "prompt_groundedness = PromptTemplate(\n",
    "    template=pr.question_groundedness_critique_prompt,\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "print(\"Question groundedness prompt:\")\n",
    "print(prompt_groundedness.template)\n",
    "\n",
    "prompt_relevancy = PromptTemplate(\n",
    "    template=pr.question_relevance_critique_prompt, input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "print(\"Question relevancy prompt:\")\n",
    "print(prompt_relevancy.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Judge to evaluate (i.e. label) the question-answer pairs based on the given document context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating each QA couple according to groundedness and question relevancy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7d110bb0894f17a4a5a441020b17ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "eval_llm = ChatOllama(model=ollama_chat[list(ollama_chat.keys())[1]], temperature=0.1)\n",
    "\n",
    "outputs = eval_qa_couples(outputs, prompt_groundedness, prompt_relevancy, eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundedness_score\n",
      "5.0    18\n",
      "3.0    11\n",
      "2.0    11\n",
      "4.0     6\n",
      "1.0     2\n",
      "NaN     2\n",
      "Name: count, dtype: int64\n",
      "question_relevancy_score\n",
      "5.0    18\n",
      "3.0    12\n",
      "4.0    12\n",
      "NaN     6\n",
      "2.0     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(generated_questions[\"groundedness_score\"].value_counts(dropna=False))\n",
    "print(generated_questions[\"question_relevancy_score\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundedness_score\n",
      "5.0    18\n",
      "3.0    11\n",
      "2.0    11\n",
      "4.0     6\n",
      "1.0     4\n",
      "Name: count, dtype: int64\n",
      "question_relevancy_score\n",
      "5.0    18\n",
      "3.0    12\n",
      "4.0    12\n",
      "1.0     6\n",
      "2.0     2\n",
      "Name: count, dtype: int64\n",
      "Generated questions saved to /Users/avosseler/Github/Privat/llm-evaluation/data/sentence_transformers_train.xlsx\n"
     ]
    }
   ],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "#generated_questions = pd.DataFrame.from_dict(outputs).dropna()\n",
    "generated_questions = generated_questions.fillna({\"groundedness_score\": 1, 'question_relevancy_score': 1})\n",
    "\n",
    "print(generated_questions[\"groundedness_score\"].value_counts(dropna=False))\n",
    "print(generated_questions[\"question_relevancy_score\"].value_counts(dropna=False))\n",
    "\n",
    "# generated_questions_good = generated_questions.loc[\n",
    "#     (generated_questions[\"groundedness_score\"] >= 4)\n",
    "#     & (generated_questions[\"question_relevancy_score\"] >= 4)\n",
    "# ]\n",
    "\n",
    "# print(generated_questions_good.shape)\n",
    "\n",
    "# # Take the complement of the good questions\n",
    "# generated_questions_bad = generated_questions.loc[\n",
    "#     ~generated_questions.index.isin(generated_questions_good.index)\n",
    "# ]\n",
    "\n",
    "# print(generated_questions_bad.shape)\n",
    "\n",
    "generated_questions[['question', \"answer\",'context', \"groundedness_score\"]].to_excel(f\"{glob.DATA_PKG_DIR}/sentence_transformers_train.xlsx\", index=False)\n",
    "print(f\"Generated questions saved to {glob.DATA_PKG_DIR}/sentence_transformers_train.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune embedding model based on your own corpus\n",
    "\n",
    "Assume you have a RAG application that uses sentence-transformer embeddings (i.e. can use any relevant HuggingFace models on the hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the generated 'labeled' data set into train/test for model finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 9) (10, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, _, _ = train_test_split(generated_questions, generated_questions.groundedness_score, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dichotomize into good and bad matches of sentence pairs (since the labels lack of discriminatory power and variance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_good = X_train[(X_train.groundedness_score >= 4) & (X_train.question_relevancy_score >= 4)]\n",
    "X_train_bad = X_train[(X_train.groundedness_score < 4) | (X_train.question_relevancy_score < 4)]\n",
    "\n",
    "X_test_good = X_test[(X_test.groundedness_score >= 4) & (X_test.question_relevancy_score >= 4)]\n",
    "X_test_bad = X_test[(X_test.groundedness_score < 4) | (X_test.question_relevancy_score < 4)]\n",
    "\n",
    "\n",
    "questions_train_good = X_train_good.question.to_list()\n",
    "context_train_good = X_train_good.context.to_list()\n",
    "#score_train_good = X_train_good.groundedness_score.to_list()\n",
    "\n",
    "questions_train_bad = X_train_bad.question.to_list()\n",
    "context_train_bad = X_train_bad.context.to_list()\n",
    "\n",
    "questions_test_good = X_test_good.question.to_list()\n",
    "context_test_good = X_test_good.context.to_list()\n",
    "\n",
    "questions_test_bad = X_test_bad.question.to_list()\n",
    "context_test_bad = X_test_bad.context.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your train/test data out of your custom dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expects as input two texts and a label of either 0 or 1 (Negatives/Positives). If the label == 1, then the distance between the two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bdf440bc78417aa1778906104b47ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a0955cbdcf4402a909580497588872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70543cc9947c4ccea0897e246ee5c2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66221d8fa1954fa58bed6fbf6afaa51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examples = []\n",
    "for sentence1, sentence2 in tqdm(zip(questions_train_good, context_train_good),total=len(context_train_good)):\n",
    "    train_examples.append(InputExample(texts=[sentence1, sentence2], label=1))                 # positive examples\n",
    "    # train_examples.append(InputExample(texts=[sentence1, sentence2]))\n",
    "\n",
    "for sentence1, sentence2 in tqdm(zip(questions_train_bad, context_train_bad),total=len(context_train_bad)):\n",
    "    train_examples.append(InputExample(texts=[sentence1, sentence2], label=0))                  # negative examples\n",
    "\n",
    "print(len(train_examples))\n",
    "\n",
    "test_examples = []\n",
    "for sentence1, sentence2 in tqdm(zip(questions_test_good, context_test_good),total=len(context_test_good)):\n",
    "    test_examples.append(InputExample(texts=[sentence1, sentence2], label=1))\n",
    "\n",
    "for sentence1, sentence2 in tqdm(zip(questions_test_bad, context_test_bad),total=len(context_test_bad)):\n",
    "    test_examples.append(InputExample(texts=[sentence1, sentence2], label=0))\n",
    "\n",
    "print(len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaed94e46b74b2ba579c888a0cc4e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df1a551f0e7480a817cae9291df7d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28.9172, 'train_samples_per_second': 138.326, 'train_steps_per_second': 13.833, 'train_loss': 0.010766148567199707, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss, MultipleNegativesRankingLoss, ContrastiveLoss\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load a model to train/finetune\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "train_dataset = SentencesDataset(train_examples, model)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=10)\n",
    "\n",
    "test_dataset = SentencesDataset(test_examples, model)\n",
    "test_dataloader = DataLoader(test_examples, shuffle=True, batch_size=10)\n",
    "\n",
    "# # Define the evaluator\n",
    "# evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples, name='my_evaluator')\n",
    "\n",
    "# # Given queries, a corpus and a mapping with relevant documents, the InformationRetrievalEvaluator computes different IR metrics.\n",
    "# evaluator = InformationRetrievalEvaluator(\n",
    "#     queries=questions_test,\n",
    "#     corpus=corpus,\n",
    "#     relevant_docs=context_test,\n",
    "#     name=\"BeIR-touche2020-subset-test\",\n",
    "# )\n",
    "\n",
    "# Initialize loss function\n",
    "#loss = CosineSimilarityLoss(model=model)\n",
    "# loss = MultipleNegativesRankingLoss(model=model)\n",
    "loss = ContrastiveLoss(model=model)\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, loss)],\n",
    "    epochs=100,\n",
    "    warmup_steps=100,\n",
    "    # evaluator=evaluator, \n",
    "    # evaluation_steps=500,\n",
    "    output_path=os.path.join(glob.DATA_PKG_DIR,\"fine-tuned-model\")\n",
    ")\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"tuned_models/fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your fine-tuned embeddings for semantic search queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the original and fine-tuned models\n",
    "original_model = SentenceTransformer(model_name)\n",
    "fine_tuned_model = SentenceTransformer(os.path.join(glob.DATA_PKG_DIR,\"fine-tuned-model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original and fine-tuned embeddings for 'What are the weekly working hours?' have a similarity of tensor([[0.8558]])\n",
      "The original and fine-tuned embeddings for 'What is the dress code?' have a similarity of tensor([[0.8725]])\n",
      "The original and fine-tuned embeddings for 'What is the vacation policy?' have a similarity of tensor([[0.9005]])\n",
      "The original and fine-tuned embeddings for 'What is the sick leave policy?' have a similarity of tensor([[0.8670]])\n",
      "The original and fine-tuned embeddings for 'What is the parental leave policy?' have a similarity of tensor([[0.8277]])\n",
      "The original and fine-tuned embeddings for 'What is the notice period?' have a similarity of tensor([[0.8509]])\n",
      "The original and fine-tuned embeddings for 'What is the probation period?' have a similarity of tensor([[0.8947]])\n"
     ]
    }
   ],
   "source": [
    "# Use the model to generate embeddings\n",
    "sentences = [\n",
    "        \"What are the weekly working hours?\",\n",
    "        \"What is the dress code?\",\n",
    "        \"What is the vacation policy?\",\n",
    "        \"What is the sick leave policy?\",\n",
    "        \"What is the parental leave policy?\",\n",
    "        \"What is the notice period?\",\n",
    "        \"What is the probation period?\",\n",
    "    ]\n",
    "\n",
    "for sentence in sentences:\n",
    "    original_embedding = original_model.encode(sentence)\n",
    "    fine_tuned_embedding = fine_tuned_model.encode(sentence)\n",
    "\n",
    "    cosine_similarity = util.pytorch_cos_sim(original_embedding, fine_tuned_embedding)\n",
    "    print(f\"The original and fine-tuned embeddings for '{sentence}' have a similarity of {cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
